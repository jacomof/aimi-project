{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a4a7a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14c68c54cc40>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14c68c5877f0, raw_cell=\"import os\n",
      "import sys\n",
      "import random\n",
      "\n",
      "sys.path.appen..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bcnlogin22.science.ru.nl/home/ldokovic/aimi-project/SegFormer3D-main/experiments/uls_2023/eval_diceCE_experiment/tta_runner.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14c68c54cc40>> (for post_run_cell), with arguments args (<ExecutionResult object at 14c68c5875e0, execution_count=23 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 14c68c5877f0, raw_cell=\"import os\n",
      "import sys\n",
      "import random\n",
      "\n",
      "sys.path.appen..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bcnlogin22.science.ru.nl/home/ldokovic/aimi-project/SegFormer3D-main/experiments/uls_2023/eval_diceCE_experiment/tta_runner.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "sys.path.append(\"SegFormer3D-main\")\n",
    "\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "from termcolor import colored\n",
    "from accelerate import Accelerator\n",
    "from losses.losses import build_loss_fn\n",
    "from optimizers.optimizers import build_optimizer\n",
    "from optimizers.schedulers import build_scheduler\n",
    "from train_scripts.trainer_ddp import Segmentation_Trainer\n",
    "from architectures.build_architecture import build_architecture\n",
    "from dataloaders.build_dataset import build_dataset, build_dataloader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cad9fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14c68c54cc40>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14c6682aa0b0, raw_cell=\"import wandb\n",
      "\n",
      "wandb.login(key=\"440fbdfe19fc3547947..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bcnlogin22.science.ru.nl/home/ldokovic/aimi-project/SegFormer3D-main/experiments/uls_2023/eval_diceCE_experiment/tta_runner.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14c68c54cc40>> (for post_run_cell), with arguments args (<ExecutionResult object at 14c6682ab220, execution_count=24 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 14c6682aa0b0, raw_cell=\"import wandb\n",
      "\n",
      "wandb.login(key=\"440fbdfe19fc3547947..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bcnlogin22.science.ru.nl/home/ldokovic/aimi-project/SegFormer3D-main/experiments/uls_2023/eval_diceCE_experiment/tta_runner.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D> result=True>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"440fbdfe19fc3547947869a7935dd2ad9028815b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb6ac63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x14c68c54cc40>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 14c626331960, raw_cell=\"%load_ext autoreload\n",
      "\n",
      "%autoreload 2\n",
      "\n",
      "#############..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bcnlogin22.science.ru.nl/home/ldokovic/aimi-project/SegFormer3D-main/experiments/uls_2023/eval_diceCE_experiment/tta_runner.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[info] -- Running in evaluation mode only.\n",
      "Initializing dataset with csv file:  ../../../data/uls2023_seg/train.csv\n",
      "Number of samples:  4907\n",
      "Loaded ULS2023 dataset\n",
      "Initializing dataset with csv file:  ../../../data/uls2023_seg/validation.csv\n",
      "Number of samples:  866\n",
      "Loaded ULS2023 dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:edkok6gr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">segformer3d_adamw_batch8_diceCEloss_continued</strong> at: <a href='https://wandb.ai/gatonegro/uls2023/runs/edkok6gr' target=\"_blank\">https://wandb.ai/gatonegro/uls2023/runs/edkok6gr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/ldokovic/aimi-project/data/wandb_logs/wandb/run-20250528_121927-edkok6gr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:edkok6gr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ldokovic/aimi-project/data/wandb_logs/wandb/run-20250528_122608-qkci9jr4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gatonegro/uls2023/runs/qkci9jr4' target=\"_blank\">segformer3d_adamw_batch8_diceCEloss_continued</a></strong> to <a href='https://wandb.ai/gatonegro/uls2023' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gatonegro/uls2023' target=\"_blank\">https://wandb.ai/gatonegro/uls2023</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gatonegro/uls2023/runs/qkci9jr4' target=\"_blank\">https://wandb.ai/gatonegro/uls2023/runs/qkci9jr4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "[info]: Experiment Info\n",
      "[info] ----- Project: \u001b[31muls2023\u001b[0m\n",
      "[info] ----- Group: \u001b[31mfirst_experiment\u001b[0m\n",
      "[info] ----- Name: \u001b[31msegformer3d_adamw_batch8_diceCEloss_continued\u001b[0m\n",
      "[info] ----- Batch Size: \u001b[31m8\u001b[0m\n",
      "[info] ----- Num Epochs: \u001b[31m400\u001b[0m\n",
      "[info] ----- Loss: \u001b[31mdiceCE\u001b[0m\n",
      "[info] ----- Optimizer: \u001b[31madamw\u001b[0m\n",
      "[info] ----- Train Dataset Size: \u001b[31m4907\u001b[0m\n",
      "[info] ----- Test Dataset Size: \u001b[31m866\u001b[0m\n",
      "[info] ----- Distributed Training: \u001b[31mFalse\u001b[0m\n",
      "[info] ----- Num Clases: \u001b[31m2\u001b[0m\n",
      "[info] ----- EMA: \u001b[31mFalse\u001b[0m\n",
      "[info] ----- Load From Checkpoint: \u001b[31mTrue\u001b[0m\n",
      "[info] ----- Params: \u001b[31m4492066\u001b[0m\n",
      "-------------------------------------------------------\n",
      "[info] -- Trainer initialized!\n",
      "[info] -- Loading checkpoint.\n",
      "[info] -- Setup complete.\n",
      "[info] -- Running evaluation only.\n",
      "\u001b[31m[info] -- starting evaluation\u001b[0m\n",
      "[info] -- Starting model evaluation\n",
      "YESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 866/866 [02:26<00:00,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss -- \u001b[32m0.00000\u001b[0m || eval mean_uls_metric -- \u001b[32m0.00000\u001b[0m -- savedeval my_mean_uls_metric -- \u001b[32m0.00000\u001b[0m -- savedeval mean_dice -- \u001b[32m0.61598\u001b[0m -- saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">segformer3d_adamw_batch8_diceCEloss_continued</strong> at: <a href='https://wandb.ai/gatonegro/uls2023/runs/qkci9jr4' target=\"_blank\">https://wandb.ai/gatonegro/uls2023/runs/qkci9jr4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/ldokovic/aimi-project/data/wandb_logs/wandb/run-20250528_122608-qkci9jr4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "##################################################################################################\n",
    "def launch_experiment(config_path) -> Dict:\n",
    "    \"\"\"\n",
    "    Builds Experiment\n",
    "    Args:\n",
    "        config (Dict): configuration file\n",
    "\n",
    "    Returns:\n",
    "        Dict: _description_\n",
    "    \"\"\"\n",
    "    # load config\n",
    "    config = load_config(config_path)\n",
    "\n",
    "    # set seed\n",
    "    seed_everything(config)\n",
    "\n",
    "    if config[\"evaluate_only\"]:\n",
    "        print(\"[info] -- Running in evaluation mode only.\")\n",
    "        if not config[\"training_parameters\"][\"load_checkpoint\"][\"load_full_checkpoint\"]:\n",
    "            raise ValueError(\"Checkpoint path must be provided for evaluation.\")\n",
    "    else:\n",
    "        # build directories\n",
    "        build_directories(config)\n",
    "\n",
    "    # build training dataset & training data loader\n",
    "    trainset = build_dataset(\n",
    "        dataset_type=config[\"dataset_parameters\"][\"dataset_type\"],\n",
    "        dataset_args=config[\"dataset_parameters\"][\"train_dataset_args\"],\n",
    "    )\n",
    "    trainloader = build_dataloader(\n",
    "        dataset=trainset,\n",
    "        dataloader_args=config[\"dataset_parameters\"][\"train_dataloader_args\"],\n",
    "        config=config,\n",
    "        train=True,\n",
    "    )\n",
    "\n",
    "    # build validation dataset & validataion data loader\n",
    "    valset = build_dataset(\n",
    "        dataset_type=config[\"dataset_parameters\"][\"dataset_type\"],\n",
    "        dataset_args=config[\"dataset_parameters\"][\"val_dataset_args\"],\n",
    "    )\n",
    "    valloader = build_dataloader(\n",
    "        dataset=valset,\n",
    "        dataloader_args=config[\"dataset_parameters\"][\"val_dataloader_args\"],\n",
    "        config=config,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    # build the Model\n",
    "    model = build_architecture(config)\n",
    "\n",
    "    # set up the loss function\n",
    "    criterion = build_loss_fn(\n",
    "        loss_type=config[\"loss_fn\"][\"loss_type\"],\n",
    "        loss_args=config[\"loss_fn\"][\"loss_args\"],\n",
    "    )\n",
    "\n",
    "    # set up the optimizer\n",
    "    optimizer = build_optimizer(\n",
    "        model=model,\n",
    "        optimizer_type=config[\"optimizer\"][\"optimizer_type\"],\n",
    "        optimizer_args=config[\"optimizer\"][\"optimizer_args\"],\n",
    "    )\n",
    "\n",
    "    # set up schedulers\n",
    "    warmup_scheduler = build_scheduler(\n",
    "        optimizer=optimizer, scheduler_type=\"warmup_scheduler\", config=config\n",
    "    )\n",
    "    training_scheduler = build_scheduler(\n",
    "        optimizer=optimizer,\n",
    "        scheduler_type=\"training_scheduler\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # use accelarate\n",
    "    accelerator = Accelerator(\n",
    "        log_with=\"wandb\",\n",
    "        gradient_accumulation_steps=config[\"training_parameters\"][\n",
    "            \"grad_accumulate_steps\"\n",
    "        ],\n",
    "    )\n",
    "    accelerator.init_trackers(\n",
    "        project_name=config[\"project\"],\n",
    "        config=config,\n",
    "        init_kwargs={\"wandb\": config[\"wandb_parameters\"]},\n",
    "    )\n",
    "\n",
    "    # display experiment info\n",
    "    display_info(config, accelerator, trainset, valset, model)\n",
    "\n",
    "    # convert all components to accelerate\n",
    "    model = accelerator.prepare_model(model=model)\n",
    "    optimizer = accelerator.prepare_optimizer(optimizer=optimizer)\n",
    "    trainloader = accelerator.prepare_data_loader(data_loader=trainloader)\n",
    "    valloader = accelerator.prepare_data_loader(data_loader=valloader)\n",
    "    warmup_scheduler = accelerator.prepare_scheduler(scheduler=warmup_scheduler)\n",
    "    training_scheduler = accelerator.prepare_scheduler(scheduler=training_scheduler)\n",
    "\n",
    "        \n",
    "\n",
    "    # create a single dict to hold all parameters\n",
    "    storage = {\n",
    "        \"model\": model,\n",
    "        \"trainloader\": trainloader,\n",
    "        \"valloader\": valloader,\n",
    "        \"criterion\": criterion,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"warmup_scheduler\": warmup_scheduler,\n",
    "        \"training_scheduler\": training_scheduler,\n",
    "    }\n",
    "\n",
    "    # set up trainer\n",
    "    trainer = Segmentation_Trainer(\n",
    "        config=config,\n",
    "        model=storage[\"model\"],\n",
    "        optimizer=storage[\"optimizer\"],\n",
    "        criterion=storage[\"criterion\"],\n",
    "        train_dataloader=storage[\"trainloader\"],\n",
    "        val_dataloader=storage[\"valloader\"],\n",
    "        warmup_scheduler=storage[\"warmup_scheduler\"],\n",
    "        training_scheduler=storage[\"training_scheduler\"],\n",
    "        accelerator=accelerator,\n",
    "    )\n",
    "\n",
    "    if config[\"training_parameters\"][\"load_checkpoint\"][\"load_full_checkpoint\"]:\n",
    "        print(\"[info] -- Loading checkpoint.\")\n",
    "        load_checkpoint(\n",
    "            config=config,\n",
    "            accelerator=accelerator,\n",
    "            storage=storage,\n",
    "            trainer=trainer,)\n",
    "\n",
    "\n",
    "    print(\"[info] -- Setup complete.\")\n",
    "\n",
    "    # run train\n",
    "    if config[\"evaluate_only\"]:\n",
    "        print(\"[info] -- Running evaluation only.\")\n",
    "        trainer.evaluate()\n",
    "    else:\n",
    "        trainer.train()\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "def seed_everything(config) -> None:\n",
    "    seed = config[\"training_parameters\"][\"seed\"]\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "def load_config(config_path: str) -> Dict:\n",
    "    \"\"\"loads the yaml config file\n",
    "\n",
    "    Args:\n",
    "        config_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Dict: _description_\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "def build_directories(config: Dict) -> None:\n",
    "    # create necessary directories\n",
    "    if not os.path.exists(config[\"training_parameters\"][\"checkpoint_save_dir\"]):\n",
    "        os.makedirs(config[\"training_parameters\"][\"checkpoint_save_dir\"])\n",
    "    \n",
    "    last_model_dir = os.path.join(\n",
    "        config[\"training_parameters\"][\"checkpoint_save_dir\"],\n",
    "        \"last_epoch_model\",\n",
    "    )\n",
    "    if not os.path.exists(last_model_dir):\n",
    "        os.makedirs(last_model_dir)\n",
    "\n",
    "\n",
    "    if os.listdir(config[\"training_parameters\"][\"checkpoint_save_dir\"]) and \\\n",
    "        not config[\"training_parameters\"][\"load_checkpoint\"][\"load_full_checkpoint\"]:\n",
    "        raise ValueError(\"checkpoint exits -- preventing file override -- rename file\")\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "def display_info(config, accelerator, trainset, valset, model):\n",
    "    # print experiment info\n",
    "    accelerator.print(f\"-------------------------------------------------------\")\n",
    "    accelerator.print(f\"[info]: Experiment Info\")\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Project: {colored(config['project'], color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Group: {colored(config['wandb_parameters']['group'], color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Name: {colored(config['wandb_parameters']['name'], color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Batch Size: {colored(config['dataset_parameters']['train_dataloader_args']['batch_size'], color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Num Epochs: {colored(config['training_parameters']['num_epochs'], color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Loss: {colored(config['loss_fn']['loss_type'], color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Optimizer: {colored(config['optimizer']['optimizer_type'], color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Train Dataset Size: {colored(len(trainset), color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Test Dataset Size: {colored(len(valset), color='red')}\"\n",
    "    )\n",
    "\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Distributed Training: {colored('True' if torch.cuda.device_count() > 1 else 'False', color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Num Clases: {colored(config['model_parameters']['num_classes'], color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- EMA: {colored(config['ema']['enabled'], color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Load From Checkpoint: {colored(config['training_parameters']['load_checkpoint']['load_full_checkpoint'], color='red')}\"\n",
    "    )\n",
    "    accelerator.print(\n",
    "        f\"[info] ----- Params: {colored(pytorch_total_params, color='red')}\"\n",
    "    )\n",
    "    accelerator.print(f\"-------------------------------------------------------\")\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "def load_checkpoint(\n",
    "        config,\n",
    "        accelerator,\n",
    "        storage,\n",
    "        trainer        \n",
    "):\n",
    "    accelerator.load_state(\n",
    "        config[\"training_parameters\"][\"load_checkpoint\"][\"load_checkpoint_path\"]\n",
    "    )\n",
    "    # load trainer state\n",
    "    trainer_state_dict = torch.load(\n",
    "        os.path.join(config[\"training_parameters\"][\"load_checkpoint\"][\"load_checkpoint_path\"],\n",
    "        \"trainer_state_dict.pkl\"),\n",
    "    )\n",
    "    for key, value in trainer_state_dict.items():\n",
    "        if hasattr(trainer, key):\n",
    "            setattr(trainer, key, value)\n",
    "        else:\n",
    "            print(f\"Key {key} not found in trainer state dict\")\n",
    "        \n",
    "    # load current scheduler\n",
    "    if trainer_state_dict[\"current_epoch\"] > config[\"warmup_scheduler\"][\"warmup_epochs\"]:\n",
    "        trainer.scheduler = storage[\"training_scheduler\"]\n",
    "    else:\n",
    "        trainer.scheduler = storage[\"warmup_scheduler\"]\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Simple example of training script.\")\n",
    "# parser.add_argument(\n",
    "#     \"--config\", type=str, default=\"config.yaml\", help=\"path to yaml config file\"\n",
    "# )\n",
    "# args = parser.parse_args()\n",
    "os.chdir('/home/ldokovic/aimi-project/SegFormer3D-main/experiments/uls_2023/eval_diceCE_experiment')\n",
    "launch_experiment('./config.yaml')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
