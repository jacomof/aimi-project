{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "from termcolor import colored\n",
    "from accelerate import Accelerator\n",
    "from losses.losses import build_loss_fn\n",
    "from optimizers.optimizers import build_optimizer\n",
    "from optimizers.schedulers import build_scheduler\n",
    "from train_scripts.trainer_ddp import Segmentation_Trainer\n",
    "from architectures.build_architecture import build_architecture\n",
    "from dataloaders.build_dataset import build_dataset, build_dataloader\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    jaccard_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path: str) -> Dict:\n",
    "    \"\"\"loads the yaml config file\n",
    "\n",
    "    Args:\n",
    "        config_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Dict: _description_\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build validation dataset & validataion data loader\n",
    "valset = build_dataset(\n",
    "    dataset_type=config[\"dataset_parameters\"][\"dataset_type\"],\n",
    "    dataset_args=config[\"dataset_parameters\"][\"val_dataset_args\"],\n",
    ")\n",
    "valloader = build_dataloader(\n",
    "    dataset=valset,\n",
    "    dataloader_args=config[\"dataset_parameters\"][\"val_dataloader_args\"],\n",
    "    config=config,\n",
    "    train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_architecture(config)\n",
    "model = model.to(\"cuda:2\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational complexity:       12.8 GMac\n",
      "Number of parameters:           4.51 M  \n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "    net = model\n",
    "    macs, params = get_model_complexity_info(\n",
    "        net, (4, 128, 128, 128), as_strings=True, print_per_layer_stat=False, verbose=False\n",
    "    )\n",
    "    print(\"{:<30}  {:<8}\".format(\"Computational complexity: \", macs))\n",
    "    print(\"{:<30}  {:<8}\".format(\"Number of parameters: \", params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model parameter count =  4511939\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, 4, 128, 128, 128)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"\\nmodel parameter count = \", pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18072064"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_memory_inference(\n",
    "    model, sample_input, batch_size=1, use_amp=False, device=0\n",
    "):\n",
    "    \"\"\"Predict the maximum memory usage of the model.\n",
    "    Args:\n",
    "        optimizer_type (Type): the class name of the optimizer to instantiate\n",
    "        model (nn.Module): the neural network model\n",
    "        sample_input (torch.Tensor): A sample input to the network. It should be\n",
    "            a single item, not a batch, and it will be replicated batch_size times.\n",
    "        batch_size (int): the batch size\n",
    "        use_amp (bool): whether to estimate based on using mixed precision\n",
    "        device (torch.device): the device to use\n",
    "    \"\"\"\n",
    "    # Reset model and optimizer\n",
    "    model.cpu()\n",
    "    a = torch.cuda.memory_allocated(device)\n",
    "    model.to(device)\n",
    "    b = torch.cuda.memory_allocated(device)\n",
    "    model_memory = b - a\n",
    "    model_input = sample_input  # .unsqueeze(0).repeat(batch_size, 1)\n",
    "    output = model(model_input.to(device)).sum()\n",
    "    total_memory = model_memory\n",
    "\n",
    "    return total_memory\n",
    "\n",
    "\n",
    "estimate_memory_inference(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-01-17 05:53:07 119795:119795 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 model_inference        39.36%     348.252ms       100.00%     884.701ms     884.701ms             1  \n",
      "                    aten::conv3d         0.20%       1.735ms        23.09%     204.275ms      11.349ms            18  \n",
      "               aten::convolution         0.05%     430.000us        23.07%     204.130ms      11.341ms            18  \n",
      "              aten::_convolution         0.03%     296.000us        23.02%     203.700ms      11.317ms            18  \n",
      "        aten::mkldnn_convolution        22.67%     200.561ms        22.71%     200.906ms      14.350ms            14  \n",
      "                     aten::copy_        13.06%     115.551ms        13.06%     115.551ms       1.179ms            98  \n",
      "                    aten::linear         0.07%     605.000us        12.99%     114.891ms       2.611ms            44  \n",
      "                     aten::addmm         1.79%      15.842ms        10.67%      94.431ms       3.373ms            28  \n",
      "                    aten::matmul         0.06%     498.000us         8.36%      73.987ms       2.312ms            32  \n",
      "                       aten::bmm         6.09%      53.876ms         6.09%      53.876ms       3.367ms            16  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 884.701ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-01-17 05:53:08 119795:119795 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-01-17 05:53:08 119795:119795 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-01-17 05:54:34 119795:119795 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-01-17 05:54:34 119795:119795 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-01-17 05:54:34 119795:119795 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        26.86%       4.067ms        99.83%      15.114ms      15.114ms       0.000us         0.00%       7.423ms       7.423ms             1  \n",
      "                                      aten::convolution         1.41%     214.000us        20.34%       3.080ms     171.111us       0.000us         0.00%       1.993ms     110.722us            18  \n",
      "                                     aten::_convolution         1.16%     175.000us        18.93%       2.866ms     159.222us       0.000us         0.00%       1.993ms     110.722us            18  \n",
      "                                           aten::conv3d         1.27%     193.000us        20.73%       3.139ms     174.389us       0.000us         0.00%       1.930ms     107.222us            18  \n",
      "                                aten::cudnn_convolution         8.88%       1.344ms        15.32%       2.319ms     231.900us       1.495ms        20.14%       1.517ms     151.700us            10  \n",
      "                                           aten::linear         2.11%     320.000us        15.77%       2.387ms      54.250us       0.000us         0.00%       1.136ms      25.818us            44  \n",
      "                                            aten::addmm         5.18%     785.000us         6.88%       1.041ms      37.179us       1.043ms        14.05%       1.043ms      37.250us            28  \n",
      "                                           aten::matmul         1.66%     251.000us         9.93%       1.503ms      46.969us       0.000us         0.00%     916.000us      28.625us            32  \n",
      "                                              aten::bmm         2.15%     326.000us         2.83%     429.000us      26.812us     808.000us        10.89%     808.000us      50.500us            16  \n",
      "                                aten::native_layer_norm         3.99%     604.000us         7.20%       1.090ms      38.929us     758.000us        10.21%     758.000us      27.071us            28  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 15.140ms\n",
      "Self CUDA time total: 7.423ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cuda:2\")\n",
    "inputs = torch.randn(1, 4, 128, 128, 128).to(\"cuda:2\")\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True\n",
    ") as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
